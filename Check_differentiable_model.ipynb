{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d53a3495",
   "metadata": {},
   "source": [
    "### Check differentiability of the code!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ce5a74",
   "metadata": {},
   "source": [
    "This notebook checks that the differentiability of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "574f6558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact\n",
    "from ml_collections import ConfigDict\n",
    "from models.ETD_KT_CM_JAX_Vectorised import *\n",
    "from filters import resamplers\n",
    "from filters.filter import ParticleFilter\n",
    "from jax import config\n",
    "from jax import grad, jit, vmap\n",
    "# import jaxopt\n",
    "\n",
    "config.update(\"jax_enable_x64\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fafea74d",
   "metadata": {},
   "source": [
    "model params:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5923e068",
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_params = ConfigDict(KDV_params_2)\n",
    "signal_params.update(E=1,method='Dealiased_ETDRK4',nx = 128,P=0,S=0)\n",
    "signal_model = ETD_KT_CM_JAX_Vectorised(signal_params)\n",
    "initial_signal = initial_condition(signal_model.x, signal_params.E, signal_params.initial_condition)\n",
    "final, all = signal_model.run(initial_signal, signal_model.nmax, None)\n",
    "final = jnp.sin(2*jnp.pi*signal_model.x) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47386f34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Loss: 9.595245452868511\n",
      "Iteration 1, Loss: 9.594527935709007\n",
      "Iteration 2, Loss: 9.593810509939779\n",
      "Iteration 3, Loss: 9.593093168507618\n",
      "Iteration 4, Loss: 9.592375904540315\n",
      "Iteration 5, Loss: 9.591658711340656\n",
      "Iteration 6, Loss: 9.590941582380722\n",
      "Iteration 7, Loss: 9.590224511296343\n",
      "Iteration 8, Loss: 9.589507491881887\n",
      "Iteration 9, Loss: 9.588790518085126\n"
     ]
    }
   ],
   "source": [
    "forward_model_params = ConfigDict(KDV_params_2)\n",
    "forward_model_params.update(E=1,method='Dealiased_ETDRK4',nx = 128,P=0,S=0)\n",
    "forward_model = ETD_KT_CM_JAX_Vectorised(signal_params)\n",
    "\n",
    "# Randomly initialize the initial conditions for the forward model\n",
    "key = jax.random.PRNGKey(0)\n",
    "random_initial_conditions = initial_signal + 0.1*jnp.sin(8*jnp.pi*signal_model.x) \n",
    "\n",
    "# Define the loss function as the L2 error between the forward model's final state and the signal model's final state\n",
    "def loss_fn(initial_conditions):\n",
    "    forward_final, _ = forward_model.run(initial_conditions, forward_model.nmax, None)\n",
    "    return jnp.linalg.norm(forward_final - final)\n",
    "\n",
    "# Compute the gradient of the loss function with respect to the initial conditions\n",
    "grad_loss_fn = grad(loss_fn)\n",
    "\n",
    "# Perform gradient descent to minimize the loss\n",
    "learning_rate = 0.001\n",
    "num_iterations = 10\n",
    "optimized_initial_conditions = random_initial_conditions\n",
    "for i in range(num_iterations):\n",
    "    loss = loss_fn(optimized_initial_conditions)\n",
    "    print(f\"Iteration {i}, Loss: {loss}\")\n",
    "    gradients = grad_loss_fn(optimized_initial_conditions)\n",
    "    optimized_initial_conditions -= learning_rate * gradients\n",
    "\n",
    "# The optimized initial conditions are now stored in `optimized_initial_conditions`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4870f291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Loss: 9.594527935709007\n",
      "Iteration 1, Loss: 9.593810509939775\n",
      "Iteration 2, Loss: 9.59309316850762\n",
      "Iteration 3, Loss: 9.592375904540315\n",
      "Iteration 4, Loss: 9.591658711340656\n",
      "Iteration 5, Loss: 9.590941582380722\n",
      "Iteration 6, Loss: 9.590224511296343\n",
      "Iteration 7, Loss: 9.589507491881882\n",
      "Iteration 8, Loss: 9.588790518085123\n",
      "Iteration 9, Loss: 9.58807358400246\n",
      "Iteration 10, Loss: 9.587356683874184\n",
      "Iteration 11, Loss: 9.586639812080037\n",
      "Iteration 12, Loss: 9.585922963134957\n",
      "Iteration 13, Loss: 9.585206131684917\n",
      "Iteration 14, Loss: 9.584489312503006\n",
      "Iteration 15, Loss: 9.583772500485626\n",
      "Iteration 16, Loss: 9.583055690648871\n",
      "Iteration 17, Loss: 9.582338878125084\n",
      "Iteration 18, Loss: 9.58162205815948\n",
      "Iteration 19, Loss: 9.580905226106875\n",
      "Iteration 20, Loss: 9.58018837742878\n",
      "Iteration 21, Loss: 9.579471507690297\n",
      "Iteration 22, Loss: 9.578754612557384\n",
      "Iteration 23, Loss: 9.578037687794131\n",
      "Iteration 24, Loss: 9.577320729260114\n",
      "Iteration 25, Loss: 9.576603732907978\n",
      "Iteration 26, Loss: 9.575886694780971\n",
      "Iteration 27, Loss: 9.57516961101072\n",
      "Iteration 28, Loss: 9.57445247781498\n",
      "Iteration 29, Loss: 9.57373529149554\n",
      "Iteration 30, Loss: 9.573018048436246\n",
      "Iteration 31, Loss: 9.572300745100948\n",
      "Iteration 32, Loss: 9.571583378031782\n",
      "Iteration 33, Loss: 9.570865943847313\n",
      "Iteration 34, Loss: 9.570148439240766\n",
      "Iteration 35, Loss: 9.569430860978487\n",
      "Iteration 36, Loss: 9.568713205898327\n",
      "Iteration 37, Loss: 9.567995470908107\n",
      "Iteration 38, Loss: 9.567277652984213\n",
      "Iteration 39, Loss: 9.566559749170136\n",
      "Iteration 40, Loss: 9.56584175657522\n",
      "Iteration 41, Loss: 9.56512367237332\n",
      "Iteration 42, Loss: 9.564405493801566\n",
      "Iteration 43, Loss: 9.563687218159242\n",
      "Iteration 44, Loss: 9.562968842806587\n",
      "Iteration 45, Loss: 9.562250365163711\n",
      "Iteration 46, Loss: 9.5615317827096\n",
      "Iteration 47, Loss: 9.560813092981036\n",
      "Iteration 48, Loss: 9.5600942935717\n",
      "Iteration 49, Loss: 9.559375382131146\n",
      "Iteration 50, Loss: 9.558656356364018\n",
      "Iteration 51, Loss: 9.557937214029089\n",
      "Iteration 52, Loss: 9.55721795293852\n",
      "Iteration 53, Loss: 9.556498570956979\n",
      "Iteration 54, Loss: 9.5557790660009\n",
      "Iteration 55, Loss: 9.55505943603776\n",
      "Iteration 56, Loss: 9.554339679085329\n",
      "Iteration 57, Loss: 9.553619793211015\n",
      "Iteration 58, Loss: 9.552899776531163\n",
      "Iteration 59, Loss: 9.552179627210412\n",
      "Iteration 60, Loss: 9.551459343461092\n",
      "Iteration 61, Loss: 9.55073892354262\n",
      "Iteration 62, Loss: 9.550018365760838\n",
      "Iteration 63, Loss: 9.549297668467576\n",
      "Iteration 64, Loss: 9.54857683005998\n",
      "Iteration 65, Loss: 9.547855848980094\n",
      "Iteration 66, Loss: 9.547134723714162\n",
      "Iteration 67, Loss: 9.54641345279234\n",
      "Iteration 68, Loss: 9.545692034788026\n",
      "Iteration 69, Loss: 9.54497046831748\n",
      "Iteration 70, Loss: 9.544248752039307\n",
      "Iteration 71, Loss: 9.543526884654023\n",
      "Iteration 72, Loss: 9.542804864903557\n",
      "Iteration 73, Loss: 9.5420826915709\n",
      "Iteration 74, Loss: 9.541360363479578\n",
      "Iteration 75, Loss: 9.540637879493305\n",
      "Iteration 76, Loss: 9.539915238515494\n",
      "Iteration 77, Loss: 9.539192439488922\n",
      "Iteration 78, Loss: 9.538469481395273\n",
      "Iteration 79, Loss: 9.53774636325473\n",
      "Iteration 80, Loss: 9.53702308412566\n",
      "Iteration 81, Loss: 9.53629964310413\n",
      "Iteration 82, Loss: 9.53557603932356\n",
      "Iteration 83, Loss: 9.534852271954378\n",
      "Iteration 84, Loss: 9.534128340203566\n",
      "Iteration 85, Loss: 9.533404243314356\n",
      "Iteration 86, Loss: 9.532679980565806\n",
      "Iteration 87, Loss: 9.531955551272436\n",
      "Iteration 88, Loss: 9.531230954783899\n",
      "Iteration 89, Loss: 9.530506190484527\n",
      "Iteration 90, Loss: 9.529781257793086\n",
      "Iteration 91, Loss: 9.529056156162236\n",
      "Iteration 92, Loss: 9.528330885078343\n",
      "Iteration 93, Loss: 9.527605444060997\n",
      "Iteration 94, Loss: 9.526879832662678\n",
      "Iteration 95, Loss: 9.526154050468367\n",
      "Iteration 96, Loss: 9.525428097095196\n",
      "Iteration 97, Loss: 9.524701972192094\n",
      "Iteration 98, Loss: 9.52397567543938\n",
      "Iteration 99, Loss: 9.523249206548389\n"
     ]
    }
   ],
   "source": [
    "# Define the loss function as the L2 error between the forward model's final state and the signal model's final state\n",
    "@jit\n",
    "def loss_fn(initial_conditions):\n",
    "    forward_final, _ = forward_model.run(initial_conditions, forward_model.nmax, None)\n",
    "    return jnp.linalg.norm(forward_final - final)\n",
    "\n",
    "# Compute the gradient of the loss function with respect to the initial conditions\n",
    "@jit\n",
    "def compute_gradients(initial_conditions):\n",
    "    return grad(loss_fn)(initial_conditions)\n",
    "\n",
    "# Perform gradient descent to minimize the loss\n",
    "@jit\n",
    "def optimize(initial_conditions, learning_rate):\n",
    "    gradients = compute_gradients(initial_conditions)\n",
    "    updated_conditions = initial_conditions - learning_rate * gradients\n",
    "    loss = loss_fn(updated_conditions)\n",
    "    return updated_conditions, loss\n",
    "\n",
    "# Initialize variables for optimization\n",
    "optimized_initial_conditions = random_initial_conditions\n",
    "learning_rate = 0.001\n",
    "num_iterations = 10\n",
    "# Optimization loop\n",
    "for i in range(num_iterations):\n",
    "    optimized_initial_conditions, loss = optimize(optimized_initial_conditions, learning_rate)\n",
    "    print(f\"Iteration {i}, Loss: {loss}\")\n",
    "\n",
    "# The optimized initial conditions are now stored in `optimized_initial_conditions`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a9acfce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Loss: 9.59524545286851\n",
      "Iteration 1, Loss: 9.58686025762773\n",
      "Iteration 2, Loss: 9.578462363619181\n",
      "Iteration 3, Loss: 9.57005079298921\n",
      "Iteration 4, Loss: 9.561624374393576\n",
      "Iteration 5, Loss: 9.553181787323501\n",
      "Iteration 6, Loss: 9.544721616761208\n",
      "Iteration 7, Loss: 9.536242417260011\n",
      "Iteration 8, Loss: 9.5277427783665\n",
      "Iteration 9, Loss: 9.519221379414871\n",
      "Iteration 10, Loss: 9.510677028024267\n",
      "Iteration 11, Loss: 9.502108684787068\n",
      "Iteration 12, Loss: 9.493515478265337\n",
      "Iteration 13, Loss: 9.484896713038799\n",
      "Iteration 14, Loss: 9.47625187474211\n",
      "Iteration 15, Loss: 9.467580639269078\n",
      "Iteration 16, Loss: 9.45888289247698\n",
      "Iteration 17, Loss: 9.450158757426728\n",
      "Iteration 18, Loss: 9.441408615521143\n",
      "Iteration 19, Loss: 9.43263310831713\n",
      "Iteration 20, Loss: 9.42383311737106\n",
      "Iteration 21, Loss: 9.415009727762197\n",
      "Iteration 22, Loss: 9.406164181324186\n",
      "Iteration 23, Loss: 9.397297823273632\n",
      "Iteration 24, Loss: 9.388412049197312\n",
      "Iteration 25, Loss: 9.37950827025752\n",
      "Iteration 26, Loss: 9.370587913356273\n",
      "Iteration 27, Loss: 9.36165244270539\n",
      "Iteration 28, Loss: 9.35270337369465\n",
      "Iteration 29, Loss: 9.343742274060249\n",
      "Iteration 30, Loss: 9.334770762787604\n",
      "Iteration 31, Loss: 9.325790511086872\n",
      "Iteration 32, Loss: 9.316803243119322\n",
      "Iteration 33, Loss: 9.307810732850587\n",
      "Iteration 34, Loss: 9.298814794161796\n",
      "Iteration 35, Loss: 9.289817261825773\n",
      "Iteration 36, Loss: 9.28081996110204\n",
      "Iteration 37, Loss: 9.27182466497553\n",
      "Iteration 38, Loss: 9.26283304134456\n",
      "Iteration 39, Loss: 9.253846596391977\n",
      "Iteration 40, Loss: 9.24486662249316\n",
      "Iteration 41, Loss: 9.235894158219674\n",
      "Iteration 42, Loss: 9.226929965145347\n",
      "Iteration 43, Loss: 9.217974522829413\n",
      "Iteration 44, Loss: 9.209028040638485\n",
      "Iteration 45, Loss: 9.200090483338904\n",
      "Iteration 46, Loss: 9.191161606549196\n",
      "Iteration 47, Loss: 9.182240997943882\n",
      "Iteration 48, Loss: 9.173328120328241\n",
      "Iteration 49, Loss: 9.164422353213611\n",
      "Iteration 50, Loss: 9.155523030216967\n",
      "Iteration 51, Loss: 9.146629470402978\n",
      "Iteration 52, Loss: 9.137741002496723\n",
      "Iteration 53, Loss: 9.128856981644619\n",
      "Iteration 54, Loss: 9.119976799044178\n",
      "Iteration 55, Loss: 9.11109988528164\n",
      "Iteration 56, Loss: 9.102225708613712\n",
      "Iteration 57, Loss: 9.09335376970721\n",
      "Iteration 58, Loss: 9.084483594502958\n",
      "Iteration 59, Loss: 9.075614726879925\n",
      "Iteration 60, Loss: 9.066746722647697\n",
      "Iteration 61, Loss: 9.057879146083561\n",
      "Iteration 62, Loss: 9.049011569774835\n",
      "Iteration 63, Loss: 9.04014357797286\n",
      "Iteration 64, Loss: 9.031274773081021\n",
      "Iteration 65, Loss: 9.022404784363587\n",
      "Iteration 66, Loss: 9.013533277547527\n",
      "Iteration 67, Loss: 9.00465996374477\n",
      "Iteration 68, Loss: 8.99578460607807\n",
      "Iteration 69, Loss: 8.98690702254793\n",
      "Iteration 70, Loss: 8.97802708401828\n",
      "Iteration 71, Loss: 8.969144706693392\n",
      "Iteration 72, Loss: 8.960259839066685\n",
      "Iteration 73, Loss: 8.951372443988442\n",
      "Iteration 74, Loss: 8.942482477160143\n",
      "Iteration 75, Loss: 8.933589863940385\n",
      "Iteration 76, Loss: 8.924694476764305\n",
      "Iteration 77, Loss: 8.915796115659179\n",
      "Iteration 78, Loss: 8.906894494225854\n",
      "Iteration 79, Loss: 8.897989233030227\n",
      "Iteration 80, Loss: 8.889079861644003\n",
      "Iteration 81, Loss: 8.88016582969219\n",
      "Iteration 82, Loss: 8.871246526358533\n",
      "Iteration 83, Loss: 8.862321307035073\n",
      "Iteration 84, Loss: 8.853389525295077\n",
      "Iteration 85, Loss: 8.84445056815005\n",
      "Iteration 86, Loss: 8.835503892558021\n",
      "Iteration 87, Loss: 8.826549061273148\n",
      "Iteration 88, Loss: 8.81758577626727\n",
      "Iteration 89, Loss: 8.808613908069011\n",
      "Iteration 90, Loss: 8.799633519476115\n",
      "Iteration 91, Loss: 8.790644882263338\n",
      "Iteration 92, Loss: 8.78164848580017\n",
      "Iteration 93, Loss: 8.77264503695162\n",
      "Iteration 94, Loss: 8.763635451254302\n",
      "Iteration 95, Loss: 8.7546208360749\n",
      "Iteration 96, Loss: 8.745602467153816\n",
      "Iteration 97, Loss: 8.736581760464851\n",
      "Iteration 98, Loss: 8.727560241535404\n",
      "Iteration 99, Loss: 8.718539514174577\n",
      "Iteration 100, Loss: 8.709521229960059\n",
      "Iteration 101, Loss: 8.700507058993384\n",
      "Iteration 102, Loss: 8.691498661636704\n",
      "Iteration 103, Loss: 8.682497660522861\n",
      "Iteration 104, Loss: 8.673505612309501\n",
      "Iteration 105, Loss: 8.664523979415717\n",
      "Iteration 106, Loss: 8.65555410304218\n",
      "Iteration 107, Loss: 8.646597179644058\n",
      "Iteration 108, Loss: 8.637654243192719\n",
      "Iteration 109, Loss: 8.628726154724342\n",
      "Iteration 110, Loss: 8.619813598909818\n",
      "Iteration 111, Loss: 8.610917085191977\n",
      "Iteration 112, Loss: 8.602036949111195\n",
      "Iteration 113, Loss: 8.59317334819476\n",
      "Iteration 114, Loss: 8.584326245886562\n",
      "Iteration 115, Loss: 8.57549537509924\n",
      "Iteration 116, Loss: 8.566680167453246\n",
      "Iteration 117, Loss: 8.557879620019639\n",
      "Iteration 118, Loss: 8.549092036564941\n",
      "Iteration 119, Loss: 8.540314492842722\n",
      "Iteration 120, Loss: 8.531541638451056\n",
      "Iteration 121, Loss: 8.522762735259215\n",
      "Iteration 122, Loss: 8.513953409294405\n",
      "Iteration 123, Loss: 8.505049121984921\n",
      "Iteration 124, Loss: 8.495844620664018\n",
      "Iteration 125, Loss: 8.485545097983598\n",
      "Iteration 126, Loss: 8.470531350940208\n",
      "Iteration 127, Loss: 8.438278374347883\n",
      "Iteration 128, Loss: 8.420185863433355\n",
      "Iteration 129, Loss: 8.410429044174448\n",
      "Iteration 130, Loss: 8.401625486170586\n",
      "Iteration 131, Loss: 8.39309555396946\n",
      "Iteration 132, Loss: 8.38466461027997\n",
      "Iteration 133, Loss: 8.376269470838787\n",
      "Iteration 134, Loss: 8.367888280454556\n",
      "Iteration 135, Loss: 8.359518252657734\n",
      "Iteration 136, Loss: 8.351164771734755\n",
      "Iteration 137, Loss: 8.34283534467849\n",
      "Iteration 138, Loss: 8.334536391286406\n",
      "Iteration 139, Loss: 8.326271888513984\n",
      "Iteration 140, Loss: 8.318043224534804\n",
      "Iteration 141, Loss: 8.309849764413332\n",
      "Iteration 142, Loss: 8.301689763333888\n",
      "Iteration 143, Loss: 8.29356142125309\n",
      "Iteration 144, Loss: 8.285464010703464\n",
      "Iteration 145, Loss: 8.277399058354362\n",
      "Iteration 146, Loss: 8.269371440078457\n",
      "Iteration 147, Loss: 8.261389873307435\n",
      "Iteration 148, Loss: 8.253465701925517\n",
      "Iteration 149, Loss: 8.245608659116499\n",
      "Iteration 150, Loss: 8.237819950037544\n",
      "Iteration 151, Loss: 8.23008728763133\n",
      "Iteration 152, Loss: 8.222389188379806\n",
      "Iteration 153, Loss: 8.214709258162651\n",
      "Iteration 154, Loss: 8.207048624594593\n",
      "Iteration 155, Loss: 8.199423298853995\n",
      "Iteration 156, Loss: 8.191848680209503\n",
      "Iteration 157, Loss: 8.184326489844082\n",
      "Iteration 158, Loss: 8.17684453463887\n",
      "Iteration 159, Loss: 8.169386123780452\n",
      "Iteration 160, Loss: 8.161940212816683\n",
      "Iteration 161, Loss: 8.154506643557598\n",
      "Iteration 162, Loss: 8.14709444497485\n",
      "Iteration 163, Loss: 8.139712841881312\n",
      "Iteration 164, Loss: 8.132359650233958\n",
      "Iteration 165, Loss: 8.125019325122768\n",
      "Iteration 166, Loss: 8.117677139624442\n",
      "Iteration 167, Loss: 8.11033411845126\n",
      "Iteration 168, Loss: 8.103002650209206\n",
      "Iteration 169, Loss: 8.095688477842206\n",
      "Iteration 170, Loss: 8.088383001673563\n",
      "Iteration 171, Loss: 8.081074031234118\n",
      "Iteration 172, Loss: 8.07375991528593\n",
      "Iteration 173, Loss: 8.066450448286108\n",
      "Iteration 174, Loss: 8.059152684445616\n",
      "Iteration 175, Loss: 8.051859663308516\n",
      "Iteration 176, Loss: 8.04456163062335\n",
      "Iteration 177, Loss: 8.037263634594451\n",
      "Iteration 178, Loss: 8.029978002279314\n",
      "Iteration 179, Loss: 8.022704782109294\n",
      "Iteration 180, Loss: 8.015436202947292\n",
      "Iteration 181, Loss: 8.008175912796311\n",
      "Iteration 182, Loss: 8.000936141220302\n",
      "Iteration 183, Loss: 7.993717004832088\n",
      "Iteration 184, Loss: 7.98651186937175\n",
      "Iteration 185, Loss: 7.979329167448804\n",
      "Iteration 186, Loss: 7.972178422102647\n",
      "Iteration 187, Loss: 7.965054164452825\n",
      "Iteration 188, Loss: 7.957958069920049\n",
      "Iteration 189, Loss: 7.950901009040375\n",
      "Iteration 190, Loss: 7.943878902590348\n",
      "Iteration 191, Loss: 7.936891102727531\n",
      "Iteration 192, Loss: 7.92994770569747\n",
      "Iteration 193, Loss: 7.923043506763058\n",
      "Iteration 194, Loss: 7.916180313255119\n",
      "Iteration 195, Loss: 7.909364011002379\n",
      "Iteration 196, Loss: 7.902587504954149\n",
      "Iteration 197, Loss: 7.895857805419725\n",
      "Iteration 198, Loss: 7.88917186368806\n",
      "Iteration 199, Loss: 7.882529547447058\n",
      "Iteration 200, Loss: 7.875933507940467\n",
      "Iteration 201, Loss: 7.869378901167736\n",
      "Iteration 202, Loss: 7.862870663544867\n",
      "Iteration 203, Loss: 7.856403063706535\n",
      "Iteration 204, Loss: 7.849980694491411\n",
      "Iteration 205, Loss: 7.843598067453603\n",
      "Iteration 206, Loss: 7.837259200200337\n",
      "Iteration 207, Loss: 7.830959483190098\n",
      "Iteration 208, Loss: 7.824701715677156\n",
      "Iteration 209, Loss: 7.818482865463843\n",
      "Iteration 210, Loss: 7.81230351853086\n",
      "Iteration 211, Loss: 7.806163635225143\n",
      "Iteration 212, Loss: 7.800060796870465\n",
      "Iteration 213, Loss: 7.793996951808088\n",
      "Iteration 214, Loss: 7.787969991171799\n",
      "Iteration 215, Loss: 7.781979345063846\n",
      "Iteration 216, Loss: 7.7760259578925215\n",
      "Iteration 217, Loss: 7.770108320632393\n",
      "Iteration 218, Loss: 7.764225542974728\n",
      "Iteration 219, Loss: 7.758377931500491\n",
      "Iteration 220, Loss: 7.752565188767437\n",
      "Iteration 221, Loss: 7.7467863501250545\n",
      "Iteration 222, Loss: 7.741040529155659\n",
      "Iteration 223, Loss: 7.735327305490664\n",
      "Iteration 224, Loss: 7.729646082318912\n",
      "Iteration 225, Loss: 7.723996700005164\n",
      "Iteration 226, Loss: 7.718379568621787\n",
      "Iteration 227, Loss: 7.712801442820568\n",
      "Iteration 228, Loss: 7.7072887171787645\n",
      "Iteration 229, Loss: 7.702036892714874\n",
      "Iteration 230, Loss: 7.696976559838372\n",
      "Iteration 231, Loss: 7.692888643206117\n",
      "Iteration 232, Loss: 7.686303178844551\n",
      "Iteration 233, Loss: 7.6951332992419825\n",
      "Iteration 234, Loss: 7.683827612593447\n",
      "Iteration 235, Loss: 7.686611270429244\n",
      "Iteration 236, Loss: 7.694076541775608\n",
      "Iteration 237, Loss: 7.702543613286729\n",
      "Iteration 238, Loss: 7.703848359585328\n",
      "Iteration 239, Loss: 7.702077681430351\n",
      "Iteration 240, Loss: 7.699241731664584\n",
      "Iteration 241, Loss: 7.695880851060188\n",
      "Iteration 242, Loss: 7.692160843256188\n",
      "Iteration 243, Loss: 7.6881288559164425\n",
      "Iteration 244, Loss: 7.683779522092111\n",
      "Iteration 245, Loss: 7.679074532669311\n",
      "Iteration 246, Loss: 7.673949806458011\n",
      "Iteration 247, Loss: 7.668324960238293\n",
      "Iteration 248, Loss: 7.6621313140983185\n",
      "Iteration 249, Loss: 7.655377866611259\n",
      "Iteration 250, Loss: 7.648241903111825\n",
      "Iteration 251, Loss: 7.641077934628959\n",
      "Iteration 252, Loss: 7.634250995950925\n",
      "Iteration 253, Loss: 7.627949902469949\n",
      "Iteration 254, Loss: 7.62217831356334\n",
      "Iteration 255, Loss: 7.616854011585607\n",
      "Iteration 256, Loss: 7.611882619538954\n",
      "Iteration 257, Loss: 7.6071786082781525\n",
      "Iteration 258, Loss: 7.6026566829899345\n",
      "Iteration 259, Loss: 7.598216890551805\n",
      "Iteration 260, Loss: 7.5937390872722785\n",
      "Iteration 261, Loss: 7.589089900250736\n",
      "Iteration 262, Loss: 7.584135300830206\n",
      "Iteration 263, Loss: 7.578773528800737\n",
      "Iteration 264, Loss: 7.573081256644906\n",
      "Iteration 265, Loss: 7.567783584583441\n",
      "Iteration 266, Loss: 7.564595346306677\n",
      "Iteration 267, Loss: 7.563016233079508\n",
      "Iteration 268, Loss: 7.56158594302077\n",
      "Iteration 269, Loss: 7.554295776627608\n",
      "Iteration 270, Loss: 7.5495015031463515\n",
      "Iteration 271, Loss: 7.546774159867617\n",
      "Iteration 272, Loss: 7.544576894086743\n",
      "Iteration 273, Loss: 7.54209973031751\n",
      "Iteration 274, Loss: 7.539192015211549\n",
      "Iteration 275, Loss: 7.535856576003178\n",
      "Iteration 276, Loss: 7.5321107140682155\n",
      "Iteration 277, Loss: 7.528007213344343\n",
      "Iteration 278, Loss: 7.523764698030665\n",
      "Iteration 279, Loss: 7.519960396407196\n",
      "Iteration 280, Loss: 7.517210065810687\n",
      "Iteration 281, Loss: 7.514960360271238\n",
      "Iteration 282, Loss: 7.512285233659312\n",
      "Iteration 283, Loss: 7.508856310504728\n",
      "Iteration 284, Loss: 7.50504473477604\n",
      "Iteration 285, Loss: 7.501535570160358\n",
      "Iteration 286, Loss: 7.498711968930182\n",
      "Iteration 287, Loss: 7.496169931470336\n",
      "Iteration 288, Loss: 7.493468867347066\n",
      "Iteration 289, Loss: 7.490452340421731\n",
      "Iteration 290, Loss: 7.487184849463313\n",
      "Iteration 291, Loss: 7.483908311426215\n",
      "Iteration 292, Loss: 7.480938061439708\n",
      "Iteration 293, Loss: 7.478319080756528\n",
      "Iteration 294, Loss: 7.475698462182843\n",
      "Iteration 295, Loss: 7.4728021576442085\n",
      "Iteration 296, Loss: 7.469720526097319\n",
      "Iteration 297, Loss: 7.46676493783226\n",
      "Iteration 298, Loss: 7.464072301949108\n",
      "Iteration 299, Loss: 7.46147883641394\n",
      "Iteration 300, Loss: 7.458794203256938\n",
      "Iteration 301, Loss: 7.4559765070062785\n",
      "Iteration 302, Loss: 7.453131873706636\n",
      "Iteration 303, Loss: 7.4504145946088345\n",
      "Iteration 304, Loss: 7.447851027186754\n",
      "Iteration 305, Loss: 7.445292542419197\n",
      "Iteration 306, Loss: 7.442626500869031\n",
      "Iteration 307, Loss: 7.439924080518148\n",
      "Iteration 308, Loss: 7.437317905076417\n",
      "Iteration 309, Loss: 7.434814212281727\n",
      "Iteration 310, Loss: 7.4323167120532565\n",
      "Iteration 311, Loss: 7.42976611653899\n",
      "Iteration 312, Loss: 7.427197136614325\n",
      "Iteration 313, Loss: 7.424687401754436\n",
      "Iteration 314, Loss: 7.4222543262862795\n",
      "Iteration 315, Loss: 7.419829424781591\n",
      "Iteration 316, Loss: 7.417366084001877\n",
      "Iteration 317, Loss: 7.414907628271911\n",
      "Iteration 318, Loss: 7.41250763267684\n",
      "Iteration 319, Loss: 7.410149120848333\n",
      "Iteration 320, Loss: 7.407784187179288\n",
      "Iteration 321, Loss: 7.405405443752147\n",
      "Iteration 322, Loss: 7.403049431264337\n",
      "Iteration 323, Loss: 7.400738783548905\n",
      "Iteration 324, Loss: 7.398446817217085\n",
      "Iteration 325, Loss: 7.3961446558130906\n",
      "Iteration 326, Loss: 7.393849557059022\n",
      "Iteration 327, Loss: 7.3915887250067565\n",
      "Iteration 328, Loss: 7.389352576107646\n",
      "Iteration 329, Loss: 7.3871171192362155\n",
      "Iteration 330, Loss: 7.384884281836538\n",
      "Iteration 331, Loss: 7.382674732975984\n",
      "Iteration 332, Loss: 7.380490454596713\n",
      "Iteration 333, Loss: 7.3783123948131975\n",
      "Iteration 334, Loss: 7.376137185527205\n",
      "Iteration 335, Loss: 7.373980857369986\n",
      "Iteration 336, Loss: 7.3718458829908995\n",
      "Iteration 337, Loss: 7.369718798092339\n",
      "Iteration 338, Loss: 7.367596887643019\n",
      "Iteration 339, Loss: 7.365491171378026\n",
      "Iteration 340, Loss: 7.363403476502508\n",
      "Iteration 341, Loss: 7.361323384951513\n",
      "Iteration 342, Loss: 7.3592501861731385\n",
      "Iteration 343, Loss: 7.357192536012889\n",
      "Iteration 344, Loss: 7.355149040055843\n",
      "Iteration 345, Loss: 7.353112362603937\n",
      "Iteration 346, Loss: 7.351084612101281\n",
      "Iteration 347, Loss: 7.349071231368542\n",
      "Iteration 348, Loss: 7.347068558485946\n",
      "Iteration 349, Loss: 7.345072442716272\n",
      "Iteration 350, Loss: 7.343087016713852\n",
      "Iteration 351, Loss: 7.341113819053852\n",
      "Iteration 352, Loss: 7.339148357332269\n",
      "Iteration 353, Loss: 7.337190643428239\n",
      "Iteration 354, Loss: 7.3352440490559765\n",
      "Iteration 355, Loss: 7.333306550250903\n",
      "Iteration 356, Loss: 7.331375730313939\n",
      "Iteration 357, Loss: 7.3294541425330735\n",
      "Iteration 358, Loss: 7.327541912506885\n",
      "Iteration 359, Loss: 7.325636349138579\n",
      "Iteration 360, Loss: 7.323738488967966\n",
      "Iteration 361, Loss: 7.321849486716548\n",
      "Iteration 362, Loss: 7.319967161348683\n",
      "Iteration 363, Loss: 7.318091563812782\n",
      "Iteration 364, Loss: 7.316224010410549\n",
      "Iteration 365, Loss: 7.314362955944828\n",
      "Iteration 366, Loss: 7.3125080296102425\n",
      "Iteration 367, Loss: 7.31066034870783\n",
      "Iteration 368, Loss: 7.308818752816612\n",
      "Iteration 369, Loss: 7.306982805562563\n",
      "Iteration 370, Loss: 7.305153385730526\n",
      "Iteration 371, Loss: 7.303329523963429\n",
      "Iteration 372, Loss: 7.3015109426401885\n",
      "Iteration 373, Loss: 7.299698238008221\n",
      "Iteration 374, Loss: 7.2978904985089805\n",
      "Iteration 375, Loss: 7.296087684353705\n",
      "Iteration 376, Loss: 7.294290100142845\n",
      "Iteration 377, Loss: 7.292496940457804\n",
      "Iteration 378, Loss: 7.290708402327763\n",
      "Iteration 379, Loss: 7.288924418842608\n",
      "Iteration 380, Loss: 7.287144389382185\n",
      "Iteration 381, Loss: 7.285368621612305\n",
      "Iteration 382, Loss: 7.283596731413421\n",
      "Iteration 383, Loss: 7.281828493114044\n",
      "Iteration 384, Loss: 7.280064043589602\n",
      "Iteration 385, Loss: 7.278302880998384\n",
      "Iteration 386, Loss: 7.276545102320092\n",
      "Iteration 387, Loss: 7.274790510875624\n",
      "Iteration 388, Loss: 7.273038856126031\n",
      "Iteration 389, Loss: 7.271290207309482\n",
      "Iteration 390, Loss: 7.269544189607439\n",
      "Iteration 391, Loss: 7.267800851910632\n",
      "Iteration 392, Loss: 7.2660599802782615\n",
      "Iteration 393, Loss: 7.264321443653601\n",
      "Iteration 394, Loss: 7.2625851886181465\n",
      "Iteration 395, Loss: 7.26085096492783\n",
      "Iteration 396, Loss: 7.2591187968724\n",
      "Iteration 397, Loss: 7.257388444232552\n",
      "Iteration 398, Loss: 7.255659896098559\n",
      "Iteration 399, Loss: 7.253932960934808\n",
      "Iteration 400, Loss: 7.252207576389646\n",
      "Iteration 401, Loss: 7.250483628589816\n",
      "Iteration 402, Loss: 7.248761012685591\n",
      "Iteration 403, Loss: 7.24703964581311\n",
      "Iteration 404, Loss: 7.245319401484115\n",
      "Iteration 405, Loss: 7.243600231472925\n",
      "Iteration 406, Loss: 7.241882012271032\n",
      "Iteration 407, Loss: 7.240164693940294\n",
      "Iteration 408, Loss: 7.238448156476011\n",
      "Iteration 409, Loss: 7.2367323629923215\n",
      "Iteration 410, Loss: 7.235017211983765\n",
      "Iteration 411, Loss: 7.233302655970007\n",
      "Iteration 412, Loss: 7.231588605089367\n",
      "Iteration 413, Loss: 7.22987501889106\n",
      "Iteration 414, Loss: 7.228161828804234\n",
      "Iteration 415, Loss: 7.22644897997755\n",
      "Iteration 416, Loss: 7.224736421967516\n",
      "Iteration 417, Loss: 7.2230241045562\n",
      "Iteration 418, Loss: 7.221311993919216\n",
      "Iteration 419, Loss: 7.219600029915638\n",
      "Iteration 420, Loss: 7.217888194074135\n",
      "Iteration 421, Loss: 7.216176439909563\n",
      "Iteration 422, Loss: 7.2144647441093905\n",
      "Iteration 423, Loss: 7.212753073067165\n",
      "Iteration 424, Loss: 7.211041404865807\n",
      "Iteration 425, Loss: 7.2093297235502\n",
      "Iteration 426, Loss: 7.207617999101996\n",
      "Iteration 427, Loss: 7.2059062266179295\n",
      "Iteration 428, Loss: 7.204194390993511\n",
      "Iteration 429, Loss: 7.202482477254826\n",
      "Iteration 430, Loss: 7.200770485110531\n",
      "Iteration 431, Loss: 7.199058406371004\n",
      "Iteration 432, Loss: 7.197346237290299\n",
      "Iteration 433, Loss: 7.19563398106311\n",
      "Iteration 434, Loss: 7.19392163754493\n",
      "Iteration 435, Loss: 7.19220920982177\n",
      "Iteration 436, Loss: 7.1904967060614755\n",
      "Iteration 437, Loss: 7.188784132550332\n",
      "Iteration 438, Loss: 7.187071496720677\n",
      "Iteration 439, Loss: 7.185358811933487\n",
      "Iteration 440, Loss: 7.183646089477121\n",
      "Iteration 441, Loss: 7.181933339882053\n",
      "Iteration 442, Loss: 7.1802205801377825\n",
      "Iteration 443, Loss: 7.178507825407018\n",
      "Iteration 444, Loss: 7.17679508988849\n",
      "Iteration 445, Loss: 7.175082391650476\n",
      "Iteration 446, Loss: 7.173369747501324\n",
      "Iteration 447, Loss: 7.171657176293468\n",
      "Iteration 448, Loss: 7.169944696522587\n",
      "Iteration 449, Loss: 7.16823232567308\n",
      "Iteration 450, Loss: 7.166520084048584\n",
      "Iteration 451, Loss: 7.164807990089511\n",
      "Iteration 452, Loss: 7.1630960641175605\n",
      "Iteration 453, Loss: 7.1613843253598946\n",
      "Iteration 454, Loss: 7.159672793214613\n",
      "Iteration 455, Loss: 7.157961487511065\n",
      "Iteration 456, Loss: 7.1562504270454985\n",
      "Iteration 457, Loss: 7.154539632018381\n",
      "Iteration 458, Loss: 7.152829120903512\n",
      "Iteration 459, Loss: 7.151118914861611\n",
      "Iteration 460, Loss: 7.1494090340800645\n",
      "Iteration 461, Loss: 7.14769950869321\n",
      "Iteration 462, Loss: 7.14599038754058\n",
      "Iteration 463, Loss: 7.144281811983082\n",
      "Iteration 464, Loss: 7.142574282727941\n",
      "Iteration 465, Loss: 7.140869750069831\n",
      "Iteration 466, Loss: 7.139177758605855\n",
      "Iteration 467, Loss: 7.137535025504131\n",
      "Iteration 468, Loss: 7.136178694242122\n",
      "Iteration 469, Loss: 7.135264231386687\n",
      "Iteration 470, Loss: 7.13723238272218\n",
      "Iteration 471, Loss: 7.1329513264073485\n",
      "Iteration 472, Loss: 7.17313876028052\n",
      "Iteration 473, Loss: 7.160122586369436\n",
      "Iteration 474, Loss: 7.212561969183314\n",
      "Iteration 475, Loss: 7.2315938753849425\n",
      "Iteration 476, Loss: 7.245486223574488\n",
      "Iteration 477, Loss: 7.255069840092387\n",
      "Iteration 478, Loss: 7.261565703333001\n",
      "Iteration 479, Loss: 7.265947496070113\n",
      "Iteration 480, Loss: 7.268829958479161\n",
      "Iteration 481, Loss: 7.2705978781620795\n",
      "Iteration 482, Loss: 7.271503177704129\n",
      "Iteration 483, Loss: 7.271718690045468\n",
      "Iteration 484, Loss: 7.271368176001892\n",
      "Iteration 485, Loss: 7.2705448290816905\n",
      "Iteration 486, Loss: 7.2693232687147376\n",
      "Iteration 487, Loss: 7.267766907469935\n",
      "Iteration 488, Loss: 7.265931898532818\n",
      "Iteration 489, Loss: 7.263868809658926\n",
      "Iteration 490, Loss: 7.261622985317393\n",
      "Iteration 491, Loss: 7.259234265054363\n",
      "Iteration 492, Loss: 7.256736498606158\n",
      "Iteration 493, Loss: 7.2541571825148745\n",
      "Iteration 494, Loss: 7.25151746644651\n",
      "Iteration 495, Loss: 7.248832663654526\n",
      "Iteration 496, Loss: 7.246113243991844\n",
      "Iteration 497, Loss: 7.243366147257424\n",
      "Iteration 498, Loss: 7.240596188524936\n",
      "Iteration 499, Loss: 7.237807346776404\n",
      "Iteration 500, Loss: 7.235003798716528\n",
      "Iteration 501, Loss: 7.232190635239032\n",
      "Iteration 502, Loss: 7.229374251799693\n",
      "Iteration 503, Loss: 7.226562433319492\n",
      "Iteration 504, Loss: 7.223764169471041\n",
      "Iteration 505, Loss: 7.2209892477005715\n",
      "Iteration 506, Loss: 7.218247683404721\n",
      "Iteration 507, Loss: 7.215549057547303\n",
      "Iteration 508, Loss: 7.21290183707488\n",
      "Iteration 509, Loss: 7.210312749062182\n",
      "Iteration 510, Loss: 7.207786265263742\n",
      "Iteration 511, Loss: 7.205324233119018\n",
      "Iteration 512, Loss: 7.202925667824669\n",
      "Iteration 513, Loss: 7.200586702857224\n",
      "Iteration 514, Loss: 7.198300685676753\n",
      "Iteration 515, Loss: 7.196058400615192\n",
      "Iteration 516, Loss: 7.193848399695152\n",
      "Iteration 517, Loss: 7.191657421807611\n",
      "Iteration 518, Loss: 7.189470879887918\n",
      "Iteration 519, Loss: 7.187273394359644\n",
      "Iteration 520, Loss: 7.185049349856934\n",
      "Iteration 521, Loss: 7.182783451887594\n",
      "Iteration 522, Loss: 7.180461261123838\n",
      "Iteration 523, Loss: 7.178069685420181\n",
      "Iteration 524, Loss: 7.175597413176245\n",
      "Iteration 525, Loss: 7.1730352759660025\n",
      "Iteration 526, Loss: 7.170376533261949\n",
      "Iteration 527, Loss: 7.167617077578369\n",
      "Iteration 528, Loss: 7.164755564398795\n",
      "Iteration 529, Loss: 7.16179347741234\n",
      "Iteration 530, Loss: 7.158735144679919\n",
      "Iteration 531, Loss: 7.155587723293728\n",
      "Iteration 532, Loss: 7.152361166253682\n",
      "Iteration 533, Loss: 7.149068173533173\n",
      "Iteration 534, Loss: 7.14572410962166\n",
      "Iteration 535, Loss: 7.142346845912747\n",
      "Iteration 536, Loss: 7.1389564662358165\n",
      "Iteration 537, Loss: 7.135574768701582\n",
      "Iteration 538, Loss: 7.132224516953669\n",
      "Iteration 539, Loss: 7.12892844212857\n",
      "Iteration 540, Loss: 7.125708064629961\n",
      "Iteration 541, Loss: 7.122582471279227\n",
      "Iteration 542, Loss: 7.119567222353089\n",
      "Iteration 543, Loss: 7.1166735554978064\n",
      "Iteration 544, Loss: 7.113907998592373\n",
      "Iteration 545, Loss: 7.111272420095159\n",
      "Iteration 546, Loss: 7.10876446239021\n",
      "Iteration 547, Loss: 7.1063782463584255\n",
      "Iteration 548, Loss: 7.104105214911837\n",
      "Iteration 549, Loss: 7.101934995403155\n",
      "Iteration 550, Loss: 7.099856192460186\n",
      "Iteration 551, Loss: 7.09785705960916\n",
      "Iteration 552, Loss: 7.09592602993431\n",
      "Iteration 553, Loss: 7.094052108460231\n",
      "Iteration 554, Loss: 7.092225141768154\n",
      "Iteration 555, Loss: 7.090435985676872\n",
      "Iteration 556, Loss: 7.088676592299498\n",
      "Iteration 557, Loss: 7.086940035712509\n",
      "Iteration 558, Loss: 7.0852204924105\n",
      "Iteration 559, Loss: 7.083513189579249\n",
      "Iteration 560, Loss: 7.08181433142874\n",
      "Iteration 561, Loss: 7.080121011522717\n",
      "Iteration 562, Loss: 7.078431117210465\n",
      "Iteration 563, Loss: 7.076743230836019\n",
      "Iteration 564, Loss: 7.075056531284712\n",
      "Iteration 565, Loss: 7.073370698551863\n",
      "Iteration 566, Loss: 7.071685823322322\n",
      "Iteration 567, Loss: 7.070002322988266\n",
      "Iteration 568, Loss: 7.068320865075057\n",
      "Iteration 569, Loss: 7.06664229866693\n",
      "Iteration 570, Loss: 7.064967594111769\n",
      "Iteration 571, Loss: 7.063297791024837\n",
      "Iteration 572, Loss: 7.061633954397646\n",
      "Iteration 573, Loss: 7.059977138444462\n",
      "Iteration 574, Loss: 7.058328357681082\n",
      "Iteration 575, Loss: 7.056688564625658\n",
      "Iteration 576, Loss: 7.055058633437001\n",
      "Iteration 577, Loss: 7.053439348759397\n",
      "Iteration 578, Loss: 7.051831399023417\n",
      "Iteration 579, Loss: 7.050235373456328\n",
      "Iteration 580, Loss: 7.048651762081562\n",
      "Iteration 581, Loss: 7.047080958030057\n",
      "Iteration 582, Loss: 7.045523261545966\n",
      "Iteration 583, Loss: 7.043978885138811\n",
      "Iteration 584, Loss: 7.042447959412218\n",
      "Iteration 585, Loss: 7.040930539180534\n",
      "Iteration 586, Loss: 7.039426609566274\n",
      "Iteration 587, Loss: 7.037936091849957\n",
      "Iteration 588, Loss: 7.036458848916336\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     14\u001b[39m     updates, opt_state = optimizer.update(gradients, opt_state)\n\u001b[32m     15\u001b[39m     optimized_initial_conditions = optax.apply_updates(optimized_initial_conditions, updates)\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIteration \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# The optimized initial conditions are now stored in `optimized_initial_conditions`\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/new_environment_python/lib/python3.13/site-packages/jax/_src/array.py:342\u001b[39m, in \u001b[36mArrayImpl.__format__\u001b[39m\u001b[34m(self, format_spec)\u001b[39m\n\u001b[32m    339\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__format__\u001b[39m(\u001b[38;5;28mself\u001b[39m, format_spec):\n\u001b[32m    340\u001b[39m   \u001b[38;5;66;03m# Simulates behavior of https://github.com/numpy/numpy/pull/9883\u001b[39;00m\n\u001b[32m    341\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ndim == \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m342\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mformat\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_value\u001b[49m[()], format_spec)\n\u001b[32m    343\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    344\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mformat\u001b[39m(\u001b[38;5;28mself\u001b[39m._value, format_spec)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/new_environment_python/lib/python3.13/site-packages/jax/_src/profiler.py:334\u001b[39m, in \u001b[36mannotate_function.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    331\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[32m    332\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args, **kwargs):\n\u001b[32m    333\u001b[39m   \u001b[38;5;28;01mwith\u001b[39;00m TraceAnnotation(name, **decorator_kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m334\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/new_environment_python/lib/python3.13/site-packages/jax/_src/array.py:641\u001b[39m, in \u001b[36mArrayImpl._value\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    639\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._npy_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    640\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_fully_replicated:\n\u001b[32m--> \u001b[39m\u001b[32m641\u001b[39m     npy_value, did_copy = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_single_device_array_to_np_array_did_copy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    642\u001b[39m     npy_value.flags.writeable = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    643\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m did_copy:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import optax\n",
    "# Define the optimizer\n",
    "learning_rate = 0.001\n",
    "optimizer = optax.adam(learning_rate)\n",
    "\n",
    "# Initialize the optimizer state\n",
    "opt_state = optimizer.init(random_initial_conditions)\n",
    "num_iterations = 1000\n",
    "# Optimization loop using Adam\n",
    "optimized_initial_conditions = random_initial_conditions\n",
    "\n",
    "for i in range(num_iterations):\n",
    "    loss, gradients = jax.value_and_grad(loss_fn)(optimized_initial_conditions)\n",
    "    updates, opt_state = optimizer.update(gradients, opt_state)\n",
    "    optimized_initial_conditions = optax.apply_updates(optimized_initial_conditions, updates)\n",
    "    print(f\"Iteration {i}, Loss: {loss}\")\n",
    "\n",
    "# The optimized initial conditions are now stored in `optimized_initial_conditions`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fd28f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot random initial conditions\n",
    "plt.plot(initial_signal[0] , label='Signal Model', linestyle='-')\n",
    "plt.plot(random_initial_conditions[0], label='Random Initial Conditions', linestyle='--',marker='x')\n",
    "\n",
    "# Plot optimized initial conditions\n",
    "plt.plot(optimized_initial_conditions[0], label='Optimized Initial Conditions', linestyle='-',marker='o')\n",
    "\n",
    "plt.title('Comparison of Initial Conditions')\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287caf7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_initial_conditions = optimized_initial_conditions\n",
    "# Run the forward model with the optimized initial conditions\n",
    "forward_final, all_rerun = signal_model.run(new_initial_conditions, forward_model.nmax, None)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(final[0], label='Final Condition', linestyle='-', marker='o')\n",
    "plt.plot(forward_final[0], label='Forward Model Final Condition', linestyle='--', marker='x')\n",
    "plt.title('Final Condition')\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.title('Error: ' + str(jnp.linalg.norm(forward_final - final)))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_environment_python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
